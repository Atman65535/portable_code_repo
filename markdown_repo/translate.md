# Mamba 使用选择性状态空间的线性时间序列建模
## 摘要：
在深度学习中为最让人激动的应用领域赋能的基础模型，几乎全都基于Transformer架构和它最核心的注意力机制。许多低于平方时间复杂度的模型比如说线性注意力，门控卷积网络，循环神经网络和结构状态空间模型已经被开发用来解决Transformer在长序列上的计算低效问题了，但是它们在一些重要的模态比如语言上，表现得没有attention机制那样好。我们查明了这些模型的一个关键弱势是它们没法去做基于内容的“询问”，并且对这个做了一些改进。第一，我们简单地让结构状态空间模型 的参数成为输入的函数，解决了他们离散模态的弱势，使得模型可以根据现有的token（令牌，专有名词翻译过来就炸了）来在输入序列的长度维度进行选择性遗忘或传播。第二， 即使这个改变避免了使用系数卷积，我们还是在循环模式里面设计了一个可以硬件并行的算法。我们结合了这些选择性结构状态空间模型到一个简单的端到端神经网络架构，完全没有注意力模块甚至多层感知机模块（Mamba）。Manba独有快速的推理速度（比Transformer快5倍），和线性尺度的序列长度，而且它的表现在将近百万序列长度的实际数据中表现更加好。作为一个通用的序列模型基础，Manba在跨越语言，音频和基因组等数个模态的任务中达到了近乎艺术的表现。在语言模型上，我们的Manba-3B模型在预训练和下游评估中都超越了同样参数大小的Transformer模型，匹敌两倍参数大小的Transformer模型。


