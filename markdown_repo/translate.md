# English Summary Response Homework

# Using AI But Not Trust it Limitlessly

From the essay Why scientists trust AI too Much-and What to Do about It, we know that the concerns focusing on AI mainly clustered in the area about the safety and reliablity of the using of AI tools. The auther regards generative AI tools as one source of risks, for its ambiguity or "hallucinate", and the opaque model of Large Language Models(LLM). And then the auther propose to using AI properly, in a carefully weighted way.The author is for sure absolutely. We should consider the risk that AI bring to us, and embrace its benifits and it brilliant efficacy.

 First of all, the risks are solid. The "AI" in most circumstances are refer to those LLMs, for instance, ChatGPT and Gemini. Those AI models are raised on large scale datas from all knowledge repository of human civilian, and they expect the sentence from the "Prompts", which you input into the dialog frame and send to AI. We all know that AI can make faults, and this warning is listed in the main interface of any published AI. The mechanism of AI is not search from some solid database and list truth to you, but a model of probablity. The AI cannot give any thing without your prompt, and the answer is highly binded to the quality of your prompt. The model calculates one kind of distance between your prompt and the texts it finally outputs, and find one candidate output that matchs your prompt best. So the output isn't truth.The other aspect of the source of the risk is the dataset for trainging. The dataset is extract and collected from libraries, internets, social dialogues and so on. So the quality is mixed. So the tone of AI can be different, from professonal to nerdy. Some times we need a precise and accurate answer, but the AI may give informal one. After all, the output of AI model containts not only truth, but potential illusions.

Another vital ideology that the author want to convey is that we should use AI carefully and rigorly. For its inaccuracy, Crockett advices that when we use AI to solve some problem that we familiar with, we are safer. That is true. But futher more, I believe that if we check the truth when we use AI, and handel the whole fluent logic chain, we can avoid the illusion of AI. Such as in the area of Computer Science, when your computer system raise a bug, you can send the message of fault to AI and get a quick fix. But you can only apply the fix to your computer after carefully validate it by reading the user manual or experiences from real human. The AI can set a direction to you, but the one that truly on the road of research, is you. We should stand solid, walking step by step, to keep us from illusion of AI.

Nowadays, AI tools are not only widely applied in academic area, but in scenes such as pychology consultation, daily chats, and education. We witness the power of AI and its versatile ablities. But the energy consumption, the illusion from AI, cheating in exams by AI and many other countless negative phenomenons that must raise our attention and serve as an alert to the AI researchers. The robust and reliablity of AI should be polished and enhanced futher.
